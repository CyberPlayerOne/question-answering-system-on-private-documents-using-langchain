{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-16T13:59:29.743996900Z",
     "start_time": "2023-08-16T13:59:29.156947869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question-Answering Pipeline\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Question-Answering Pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Prepare the document (once per document)\n",
    "a) Load the data into LangChain Documents;\n",
    "b) Split the documents into chunks;\n",
    "c) Embed the chunks into numeric vectors;\n",
    "d) Save the chunks and the embeddings to a vector database\n",
    "\n",
    "2. Search (once per query)\n",
    "a) Embed the user's question;\n",
    "b) Using the question's embedding qand the chunk embedding, rank the vectors by similarity to the question embedding. The nearest vectors represent chunks similar to the question.\n",
    "\n",
    "3. Ask (once per query)\n",
    "a) Insert the question and the most relevant chunks into a message to a GPT model;\n",
    "b) Return GPT's answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
